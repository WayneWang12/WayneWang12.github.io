{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"","title":"Wayne的博客"},{"location":"/index.html#wayne的博客","text":"","title":"Wayne的博客"},{"location":"/index.html#简介","text":"欢迎来到我的博客。本网站使用Lightbend Paradox构建，主要内容会与Scala技术栈相关。偶尔也会穿插一些琐话。欢迎关注我的公众号以及时获得更新信息。","title":"简介"},{"location":"/index.html#目录","text":"2018年 使用Paradox构建文档网站 使用Git的一些原则 反应式流回压的原理 第四届阿里中间件性能大赛初赛RDP飞起来团队总结 应用JMH测试大型HashMap的性能","title":"目录"},{"location":"/2018/index.html","text":"","title":"2018年"},{"location":"/2018/index.html#2018年","text":"使用Paradox构建文档网站 使用Git的一些原则 反应式流回压的原理 第四届阿里中间件性能大赛初赛RDP飞起来团队总结 应用JMH测试大型HashMap的性能","title":"2018年"},{"location":"/2018/paradox_site_setup.html","text":"","title":"使用Paradox构建文档网站"},{"location":"/2018/paradox_site_setup.html#使用paradox构建文档网站","text":"","title":"使用Paradox构建文档网站"},{"location":"/2018/paradox_site_setup.html#lightbend-paradox简介","text":"Lightbend Paradox是为了软件项目文档而开发的Markdown文档工具。它提供了开箱即用的基本样式、完善的Github Flavor Markdown语法支持，并且可以通过指定的方法进行代码片段引入。结合sbt-site插件，可以非常轻松地做到在本地一键发布博客的过程，或者是通过Github的Travis CI在commit之后进行自动构建发布。对于Scala程序员来说，Paradox最大的好处在于可以使得代码和文档不被割裂。你的文档可以直接放在项目里面，然后通过特定语法将代码引入到文档中，从而使得文档内容可以在IDE里面随着代码变更直接变动，而不需要上下文切换到浏览器这个新环境中进行编写。\n上周我将Paradox引入了自己正在开发的项目里，并且第一次感受到了写文档的乐趣。之后便萌生想法，想要使用Paradox来构建自己的博客。当然，对于博客来说，这个组件并非最好的。但是用来做一些基本的思考记录还是足够的。Paradox最好的归宿是用来做文档网站，结合Github pages，可以轻松地做到版本相关的文档发布。目前基本上Lightbend除了基于Playframework构建的项目是使用早就存在的PlayDocPlugin之外，其他相关的项目都是使用的Paradox来做文档管理，如Akka，Alpakka等，尤其是一些小项目，结合Github pages，相当方便。本篇将介绍如何使用Paradox快速构建文档项目。","title":"Lightbend Paradox简介"},{"location":"/2018/paradox_site_setup.html#sbt配置paradox","text":"在project/plugins.sbt中添加插件：\naddSbtPlugin(\"com.lightbend.paradox\" % \"sbt-paradox\" % \"0.4.0\")\nFull source at GitHub\n在build.sbt中定义项目：\nlazy val docDemo = (project in file(\"docs\")).\n  enablePlugins(ParadoxPlugin).\n  settings(\n    name := \"Demo Project\",\n    paradoxTheme := Some(builtinParadoxTheme(\"generic\"))\n  )\nFull source at GitHub\n之后，你只需要在docs/src/main/paradox中创建index.md文档即可：\n# 示例文档\n\n本文档由Paradox生成。\n\n## 这是会出现在右侧的二级标题\nFull source at GitHub\n然后执行sbt命令paradox，就能在docs/target/paradox/site/main中找到index.html文件，以及其他css和js文件。之后直接浏览器打开index.html即可:\n如果你拿这个页面对比一下Alphakka的文档样式，你会发现两者几乎完全一样，只是Alphakka多了一个Akka的标志而已。\nParadox的所有源文件都是使用的Markdown语法来书写。并且因为与Github结合紧密，所以其采用了Github-flavored风格。它背后的Markdown解析器是由Mathias开发的Pegdown(这哥们儿最有名的库是后来发展成为Akka-http的Spray)。当然，虽然这个库已经不再维护了，但是用来做这里的语法解析也是绰绰有余的。\n到现在我们的文档网站已经做好了。之后便需要按照需求来组织一下页面即可。","title":"Sbt配置Paradox"},{"location":"/2018/paradox_site_setup.html#页面组织","text":"所有页面的第一个标题会成为该页面的标题，不管该标题的级别是什么。页面树是通过在Markdown文档中嵌入用@@@ index容器包裹的内容来解析的。格式如下：\n@@@ index\n\n* [设置](setup/index.md)\n* [用途](usage/index.md)\n\n@@@\nFull source at GitHub\n这里的页面是一个树状的结构。你可以在后面的setup/index.md中继续使用相同的语法进行页面扩展。但是左侧的页面树默认最多只有两级。而且，如果不做特别处理的话，在当前页面是看不到页面树的。不过你可以通过在页面中添加指令\n@@toc { depth=1 }\nFull source at GitHub\n如此之后，在当前页面就也能看到页面层次了。其中depth的设置可以改变页面树的级数。\n以此类推，我们就能构建组织出一个文档项目的完整、清晰的结构。","title":"页面组织"},{"location":"/2018/paradox_site_setup.html#片段引入","text":"Github的Markup有个著名的issue#346，其中维护者明确地拒绝了在Github Markdown中添加嵌入文件片段的请求，而不顾几百个+1的请求。这个需求说实话，确实是个刚需，所以Paradox为我们贴心地做了一下实现。通过以下语法，可以将位于src/main/scala/HelloApp.scala里面的包含在// #hello-example中间的代码片段引入本文件：\n@@snip [HelloApp.scala](../../scala/HelloApp.scala) { #hello-example }\n代码源文件为：\n// #hello-example\nobject HelloApp extends App {\n  println(\"Hello, world!\")\n}\n// #hello-example\n页面中则是如下格式：\nobject HelloApp extends App {\n  println(\"Hello, world!\")\n}\nFull source at GitHub\n注意，不管是什么样的文件，只要文件名、路径对上，Paradox都能帮我们做引入，例如我上面的markdown文档片段就是直接从本docDemo项目的文档中引入进行来的。\n至此，我基本把Paradox里面的主要会用到的内容讲完了。后续如果需要做更多定制化的内容，大家可以直接去看官网文档，包括如何使用本博客采用的主题方式\n接下来我们讲如何使用sbt-site插件","title":"片段引入"},{"location":"/2018/paradox_site_setup.html#sbt-site插件的使用","text":"这些程序员一般都很懒，能代码做掉的事情绝对不手动去做。如果我们只使用paradox的话，我们需要执行sbt命令paradox之后，将生成在target/paradox/site/main里面的文件手动git add和commit到Github上面去，才能发布网站。而为了解决这个多好多步的问题，我们使用sbt-site插件，来让我们只需要执行ghpagesPushSite命令，即可一键发布。\n首先还是添加插件。在project/plugins.sbt中添加：\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-site\" % \"1.3.2\")\n\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-ghpages\" % \"0.6.2\")\nFull source at GitHub\n其中sbt-site是帮助我们打包网站，sbt-ghpages则帮助我们发布到Github的gh-pages分支。\nNote 注意，加入sbt-site插件之后，之前Paradox指引的类似在build.sbt文件中添加到Compile的属性现在要添加到Paradox才能生效，如： paradoxProperties in Paradox ++= Map(\n  \"github.base_url\" -> s\"https://github.com/WayneWang12/Waynewang12.github.io/tree/blog\",\n  \"snip.reactive-stream-demo.base_dir\" -> s\"${(baseDirectory in ThisBuild).value.getAbsolutePath}/reactive-stream-demo/src/test/scala/io/github/waynewang12\",\n  \"snip.project.base_dir\" -> (baseDirectory in ThisBuild).value.getAbsolutePath\n),Full source at GitHub 另外，为了使我们的源文件还是在src/main/paradox里面，你需要在项目中进行如下设定： sourceDirectory in Paradox := sourceDirectory.value / \"main\" / \"paradox\",\n\nsourceDirectory in Paradox in paradoxTheme := sourceDirectory.value / \"main\" / \"paradox\" / \"_template\",\nFull source at GitHub 否则，默认的源文件夹是src/paradox。\n之后，我们需要在build.sbt文件中设置发布文档项目到的目的仓库。例如，本博客的设置为：\nscmInfo := Some(ScmInfo(url(\"https://github.com/WayneWang12/WayneWang12.github.io.git\"), \"git@github.com:WayneWang12/WayneWang12.github.io.git\")),\ngit.remoteRepo := scmInfo.value.get.connection,\nname := \"Wayne's Blog\",\nghpagesBranch := \"master\",\nFull source at GitHub\n注意最后一行的ghpagesBranch := \"master\"。在Github上形如username.github.io的仓库默认是个人主页的主项目，所以这个项目中无法设置gh-pages这个分支，而必须是master。所以这里我做了改变。但是如果是其他项目的话，则可以不加这一行。例如，假如我有一个sbt-site的项目，在gh-pages分支设置好了以后，我就可以通过http://waynewang12.github.io/sbt-site来访问它了。\n上述设置完成以后，我们接下来需要做一次初始化设置：\nsbt clean make-site                             # <1> Build the site\norigin=$(git remote get-url origin)             # <2> Save the current remote for later\ncd target/site\ngit init                                        # <3> Create site repo and add initial content\ngit add .\ngit commit -m \"Initial import of GitHub Pages\"\ngit push --force \"$origin\" master:gh-pages      # <4> Publish the repo's master branch as gh-pages\n注意，如上所说，最后一行的gh-pages你需要根据你的项目性质做改变。例如，本博客的源代码就在blog分支，而Github pages内容则在master分支。所以第四步我执行的是：\ngit push --force \"$origin\" blog:master      # <4> Publish the repo's master branch as gh-pages\n这一次完成了之后，每次我需要发布新内容，就只需要执行sbt命令\nghpagesPushSite\n就可以发布新内容了。","title":"sbt-site插件的使用"},{"location":"/2018/paradox_site_setup.html#总结","text":"怎么样，对于基于sbt构建项目的Scala程序员来说，是不是很简单，很方便？最棒的一点是你可以将编译通过的代码直接引入到文档里面去，之后就算代码做了变更，文档内容也会随之变更，而不会出现代码改了文档没改的尴尬情形。并且因为所有的一切都在IDE里面完成，程序员完全不需要进行场景切换，所以效率会更高，也更有动力去编写文档。\n所以，十分推荐有文档需求的Scala程序员来尝试一下本插件。并且自行琢磨出一套文档和代码一起持续演进的流程，来与大家分享。","title":"总结"},{"location":"/2018/workingWithGit.html","text":"","title":"使用Git的一些原则"},{"location":"/2018/workingWithGit.html#使用git的一些原则","text":"这个指南是用来协助开发人员来约定如何使用Git的。 里面提到的一些内容是我们觉得会有助于开发人员更好地协作的。 但是这些约定并不是强制的，你应该使用最适合你的方式。","title":"使用Git的一些原则"},{"location":"/2018/workingWithGit.html#git远程","text":"我们推荐这样的约定：将你要贡献代码的远程仓库称为origin， 而用你的用户名来命名你从这个仓库fork出来的远程仓库。 这个约定在你从多个fork之间共享代码的时候会非常管用。而我们会在整个指南里面贯彻这个约定。这也是 GitHub命令行工具开箱即有的运行最好的约定。","title":"Git远程"},{"location":"/2018/workingWithGit.html#分支","text":"正常来说，所有工作都应该在分支上面完成。 如果你直接在master上面工作的话，那么在同一时间里，你就只能提一个pull request(Gitlab中称为merge request), 因为如果你试图从master上面提交第二个的时候， 第二个就会包含你的第一个和第二个commit。 在分支上工作允许你将pull request从彼此之间分离开来。 至于如何称呼你自己的分支那就取决于你自己了。 一些人喜欢在分支名里面包含issue号，也有些人喜欢使用等级结构来进行命名。","title":"分支"},{"location":"/2018/workingWithGit.html#squash-压缩-提交","text":"我们倾向于每一个pull request都是一个单独的提交。理由如下：\n回滚单个提交到稳定分支比回滚一组提交更加简单，并且出错的概率更小。 如果变更只在一个提交里面 ，回滚就根本没有机会污染分支，不管整个变更是不是cherry pick过来的。 我们的目标在于master分支永远都是可以发布的状态， 而这包括历史上的每个点都是可发布，而不仅仅是现在而已。 如果我们需要回滚某个提交的话，我们必须确信在这个提交之前的分支状态是稳定并且可发布的。 如果变更都是在一个提交里面的话，这有助于我们在历史纪录里面回溯变更的内容。\n当然，也存在着不适合使用squash的场景，但是这些都应该根据场景进行逐个分析。 下面列举了一些我们并不要求压缩提交的例子：\n当pull request里面包含着多于一个人的提交的时候。在这个场景里面，我们倾向于同一个人的所有提交都是squash的。 当pull request来自于被社区内部共享的fork或者分支的时候。这时候如果重写变更记录会导致从这个fork或者分支拉取变更的人遇到问题。 当pull request包含非常大的工作量的时候，提交记录会有助于理解工作的演化。\n但是，在大多数情况下，如果你的pull request包含多个提交的话，那么你需要按照如下步骤进行操作：\n确保你的repo包含核心master分支的所有变更： git fetch origin\n 开始进行一个交互式的rebase: git rebase -i origin/master\n 这个命令会在你的编辑器里面打开一个屏幕，允许你讲述你在每个提交里面做了什么。如果第一个提交的提交信息适用于描述所有提交，那么就可以保留它，否则就将它之前的命令从pick改成reword. 对于剩下的每个提交，将命令从pick改成fixup。这会告诉git将每一个提交合并到之前的提交里面去，并使用之前提交的commit message. 保存文件之后退出编辑器。Git会开始rebase的操作。如果你reword了第一个提交，那么git会给你打开一个编辑器来让你填入新的提交信息。如果填完了以后一切正常，那么工作就完成了。但是在将你的变更应用到最新的master分支的时候，可能会发生冲突，那么这个时候你就需要解决冲突，然后运行命令：git rebase --continue。如果含有冲突的变更很多，那么也许这个步骤需要被重复很多次。 好了，现在你应该已经rebase好了，这个时候就要推送你的所有变更了。如果你在squash之前已经提交过了分支（包含你已经创建了pull request的场景），那么你就需要做一个强制推送。可以通过如下命令来完成： git push yourremote yourbranch --force","title":"Squash（压缩）提交"},{"location":"/2018/workingWithGit.html#响应评审意见或者构建失败","text":"如果你的pull request没有通过CI构建，或者如果我们在代码评审之后告知你需要更新你的pull request，或者如果有任何其他你需要更新你的pull request的理由，那么与其创建一个新的提交，不如改进已有的。这个操作可以通过提交的时候使用--amend标志来完成：\ngit commit --amend\namend完成之后，我们又一次需要通过强制提交标志--force来进行提交：\ngit push yourremote yourbranch --force","title":"响应评审意见或者构建失败"},{"location":"/2018/workingWithGit.html#重新开始","text":"有时有人发现他们的pull request完全错了，然后想重新开始。这样也可以，但是没有必要关闭原来的pull request然后起一个新的。你可以使用强制推送来将一个全新的分支推送到这个pull request里面。\n重新开始之前，确保你已经获取核心仓库的最新变更，然后从那一个点创建一个新的分支：\ngit fetch origin\ngit checkout -b mynewbranch origin/master\n然后在这个分支上进行你的工作。当你准备好了要提交pull request的时候，假设你的老的分支名是myoldbranch，将你的新的分支强制推送到你的仓库里面的老的分支里面去：\ngit push yourremote mynewbranch:myoldbranch --force\n这之后，pull request会更新成你的新的分支。","title":"重新开始"},{"location":"/2018/workingWithGit.html#关于变更记录","text":"也许你听说过这么一个说法，就是你不应该在发布之后修改git的历史纪录。但是使用rebase和commit --amend都会修改变更记录，而使用push --force会发布你的整个变更记录。\n当然存在着发布之后不应该修改git历史纪录的情况，尤其是在其他人fork了你的仓库，或者从你的仓库拉取了更新的时候。 这个时候修改变更记录的话会使得其他人没有办法安全地将你在仓库里面的变更合并到他们自己的仓库里面去。基于这个理由，需要被贡献代码的核心仓库永远不应该修改自己的历史纪录。\n然而，当你在自己的私人fork里面的时候，或者在只是为了pull request而创建的分支上时，情况就完成不一样了。 这个工作流程的本质是你的变更在合并到master分支的时候才是真正地“被发布”。在此之前，你想怎样折腾你的分支都是可以的。\n当然，如果你的分支是很多人协作的，并且你相信其他人有很好的理由从你的分支进行拉取。那么这个时候请让管理核心开发库的人员悉知。这个时候，就不应该强制要求你的pull request是压缩了的提交，因为这样做并不合理。","title":"关于变更记录"},{"location":"/2018/pricinpleOfReactiveStream.html","text":"","title":"反应式流回压的原理"},{"location":"/2018/pricinpleOfReactiveStream.html#反应式流回压的原理","text":"反应式编程模型是目前编程世界的大势所趋。Java服务框架中的扛把子Spring会在5.0中全面拥抱反应式编程模型，这意味着广大的Java程序员都开始有可能接触反应式编程的思想，除非他们的项目永远被三体人锁死在了Spring 4。新版本的Spring全面拥抱了反应式宣言(该宣言中文版由何品主要翻译，我和Hawstein、CTAO、Neo一起校审，并作为附录存于《反应式设计模式》一书中。书作者Roland Kuhn是最早的起草人之一)中的思想，并且提供从Threaded Server转向Event-based Server的途径，并且反应式流会在框架中大量应用。很多人也许很早就听说过Rx Extensions，知道Rx .Net, RxJs, RxJava，RxScala，但是大部分也许对其中的模式和原理不太了然。为什么加个Rx就比我们平时用的集合框架高级了呢？反应式流的什么特性令其独树一帜了？这一篇里面，我们来揭示一下反应式流的原理。至于反应式编程及其设计模式，等我们翻译的《反应式设计模式》出版之后，我会第一时间给大家推送相关信息，敬请期待。","title":"反应式流回压的原理"},{"location":"/2018/pricinpleOfReactiveStream.html#什么是反应式流","text":"首先要明确一点，反应式流只是进行流处理的一种方式而已。它并不局限于是否使用了一种反应式相关的框架，而是你处理反应式流的方式。如果你用一种带回压的非阻塞模式进行流式处理，那么你就应用了反应式流。如下是来自反应式流官网的定义：\nReactive Streams is an initiative to provide a standard for asynchronous stream processing with non-blocking back pressure.\n你可以看到，反应式流只是一个倡议。它提议，要提供一种标准的、应用了非阻塞的回压机制的异步流式处理方法。并且，经过社区的持续努力，在JDK 9中，实现了和反应式流等价的接口java.util.concurrent.Flow","title":"什么是反应式流"},{"location":"/2018/pricinpleOfReactiveStream.html#什么是回压","text":"回压的英文是 back pressure。传统行业以及早期的翻译都是“背压”。但是这个翻译由于无法体现其具体的意思，所以我们在翻译《反应式宣言》和《反应式设计模式》的时候将其定为“回压”。记忆以及理解它相比“背压”就简单多了，就是当下游处理单元无法及时处理上游发出来的数据时，就对上游发回一个压力信息，这就是“回压”。当回压产生的时候，上游就应该放慢其发送数据的速度，以保证系统在一个合理的、可以承受的速度下运转，避免因为数据速率产生过快而导致系统崩溃。而由于处理的过程是异步的，如果瓶颈是在下游的处理单元，那么上游空出的资源就可以用来执行其他任务，从而保障了系统资源利用率的提升。传说淘宝在上一年度进行反应式架构升级之后，其内部应用服务器的资源使用率降低非常明显。","title":"什么是回压"},{"location":"/2018/pricinpleOfReactiveStream.html#流式处理面临的问题","text":"一般来说，我们在处理流式数据的时候，面临的问题可能有如下几个：\n无法确定大小和速度的活跃数据流； 数据跨越异步边界（独立的生产者到独立的消费者）； 生产者和消费者的速率不一致；\n由之而衍生的问题可以罗列如下：\n难以预知数据流对处理资源的需求程度； 异步编程问题； 分为两种情形； 如果生产者的速率快于消费者，消费者最终可能被生产者压垮； 如果消费者的速率快于生产者，消费者需要不时耗费资源来轮询生产者。\n那么，回压如何解决这些问题呢？我们来分情况罗列。","title":"流式处理面临的问题"},{"location":"/2018/pricinpleOfReactiveStream.html#速率可控的生产者","text":"速率可控的生产者的特性在于，它可以按照消费者的要求给它们发送任务。在这个场景里面我们设定如下任务：\n利用Akka的Actor来计算调和级数: 1 - 1/2 + 1/3 - 1/4 … (其值向2的自然对数收敛，近似值为0.693147180559945…) 设置一个管理者Actor负责分发计算任务和符号； 由工作者Actor来执行相对耗时的浮点数计算。\n这里，我们会引入一种模式，称为 拉取模式 :\n让消费者向生产者对数据的批量大小提出要求。\n意思就是说，由消费者来决定生产者应该给与其多少数据，而不是由生产者来决定。那么，在Akka的Actor模型下如何实现这个模式呢？我们先来看下具体的逻辑流程：\n工作者Actor启动之后，向管理者发起工作请求。每个工作请求请求10条数据； 管理者Actor根据请求的数据多少，将工作发回给工作者； 当工作者已经请求但是未接收的工作数据少于5个的时候，就再次发送10个请求 如此往复，直到达到指定精度，或者完成所有工作请求。\n首先，我们先定义相关的样例类：\ncase class Job(id: Long, input: Int, replyTo: ActorRef)\n\ncase class JobResult(id: Long, report: BigDecimal)\n\ncase class WorkRequest(worker: ActorRef, items: Int)\nFull source at GitHub\nJob是由管理者下发给工作者的内容，JobResult是计算出来的结果，WorkRequest是工作者发回给管理者的工作请求。其中包含着工作者自己的ActorRef，和请求的项目数量。\n于是，工作者逻辑可以如下实现：\nclass Worker(manager: ActorRef) extends Actor {\n  private val mc = new MathContext(100, RoundingMode.HALF_EVEN)\n  private val plus = BigDecimal(1, mc)\n  private val minus = BigDecimal(-1, mc)\n\n  private var requested = 0\n\n  def request(): Unit =\n    if (requested < 5) {\n      manager ! WorkRequest(self, 10)\n      requested += 10\n    }\n\n  request()\n\n  def receive: Receive = {\n    case Job(id, data, replyTo) ⇒\n      requested -= 1\n      request()\n      val sign = if ((data & 1) == 1) plus else minus\n      val result = sign / data\n      replyTo ! JobResult(id, result)\n  }\n}\nFull source at GitHub\n在启动的时候，就调用request()函数请求任务，之后，每当已经请求但是未处理的任务少于5个时，就会又一次发送工作请求给管理者，要求发送更多的任务过来。\n而管理者的代码则是如下：\nclass Manager(startTime: Long) extends Actor {\n\n  private val workStream: Iterator[Job] =\n    Iterator range(1, 1000000) map (x ⇒ Job(x, x, self))\n\n  private val mc = new MathContext(10000, RoundingMode.HALF_EVEN)\n  private var approximation = BigDecimal(0, mc)\n\n  private var outstandingWork = 0\n\n  (1 to 8) foreach (_ ⇒ context.actorOf(Props(new Worker(self))))\n\n  def receive: Receive = {\n    case WorkRequest(worker, items) ⇒\n      workStream.take(items).foreach { job ⇒\n        worker ! job\n        outstandingWork += 1\n      }\n    case JobResult(id, result) ⇒\n      approximation = approximation + result\n      outstandingWork -= 1\n      if (outstandingWork == 0 && workStream.isEmpty) {\n        println(s\"final result: $approximation, time spent with actor is ${System.currentTimeMillis() - startTime}ms\")\n        context.system.terminate()\n      }\n  }\n}\nFull source at GitHub\n如此，只需要生成管理者Actor，我们的计算任务就可以开始了。\nimport scala.concurrent.duration._\n\nval actorSystem = ActorSystem(\"test\")\n\nactorSystem.actorOf(Props(new Manager(System.currentTimeMillis())))\n\nAwait.result(actorSystem.whenTerminated, 1.minute)\nFull source at GitHub\n最终我们会得到结果：\nfinal result: 0.693147680560195309417231996458176568325500134359192754120687759493393535594694716971113326967369656239812, time spent is 1471ms\n读者如果有兴趣的话，可以思考下如果直接将任务分布给8个消费者的代码应该如何写。在这种情况下，也许会产生两种问题：\n消费者的缓冲队列是无界的话，在数据量大的情况下可能会导致内存耗尽； 消费者的队列如果是有界的阻塞队列，那么在消费者速率较慢的时候，可能会导致生产者的阻塞。\n另外，还需要思考的是，应该按照怎样的数量分布任务给消费者。因为预先决定的平均分布很有可能得不到平均的执行情况，有的消费者可能会快一点，有的消费者可能会慢一点。平均分布的话，最终的执行时间可能取决于最慢的消费者的消费速率。\n而且这种分布，假设其中一个工作者失败了，那么所有分配给它的任务都会丢失。此时的恢复过程就比上述的模式更加复杂。读者可以思考一下，如何分别给上述的拉取模式和这种模式执行故障恢复。","title":"速率可控的生产者"},{"location":"/2018/pricinpleOfReactiveStream.html#拉取模式的优点","text":"使用拉取模式，我们可以实现如下好处：\n当生产者快于消费者的时候，生产者最终会缺少工作请求，此时只要消费者发出工作请求，其需求就能立即得到满足，整个系统运行得就好像在拉取状态； 当生产者慢于消费者的时候，消费者最终会有违背满足的需求，此时只要生产者产生了任务，就能立即发送给消费者，整个系统运行得就好像在推送状态； 在负载一直变化的情况下，这个机制将会自动地在前述两种模式下切换，而不需要任何额外的协调工作。整个系统运行的就好像在一种动态推拉的状态下； 只要能确定消费者的数量，这个过程即使是异步的，它也不会需要任何的加锁机制。\n在这个过程中，回压体现在哪里呢？我们可以看到，当消费者无法处理任务的时候，生产者也不会向其发送多余的任务，因为消费者无力向生产者发送更多的工作请求。所以这里的回压是通过工作请求的缺失来体现的。\n进一步，我们假设生产者只是一个中间节点，其任务是由上游的生产者传递给它的，并且它也按照上述方法实现了回压机制，则由上游至中间节点至消费者的过程中，这一整个链条都能实现回压。消费者回压中间节点，中间节点回压上游生产者。所以，由实现了回压机制的处理单元组合成的流程，回压机制可以在整个流程上传递。所以，整个系统都能实现动态推拉的特性，使得系统健壮而高效。","title":"拉取模式的优点"},{"location":"/2018/pricinpleOfReactiveStream.html#akka-stream实现一版上述逻辑","text":"Akka Stream是反应式流的一种实现。其内部的处理单元都良好地实现了回压机制。我用Akka Stream实现了一版本的上述功能，代码如下：\nimplicit val actorSystem: ActorSystem = ActorSystem(\"test\")\nimplicit val materializer: Materializer = ActorMaterializer()\n\nval mc = new MathContext(100, RoundingMode.HALF_EVEN)\nval plus = BigDecimal(1, mc)\nval minus = BigDecimal(-1, mc)\n\nval producer: Source[Int, NotUsed] = Source.fromIterator(() => Iterator.range(1, 1000000))\n\nval consumers: Flow[Int, BigDecimal, NotUsed] = Flow.fromGraph(GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n  val balancer = builder.add(Balance[Int](8))\n  val worker = Flow[Int].map { data =>\n    val sign = if ((data & 1) == 1) plus else minus\n    val result = sign / data\n    result\n  }\n\n  val merge = builder.add(Merge[BigDecimal](8))\n  val workers = List.fill(8)(worker)\n\n  workers.foreach(f =>\n    balancer ~> f ~> merge\n  )\n  FlowShape(balancer.in, merge.out)\n})\n\nval resultAggregator = Sink.fold[BigDecimal, BigDecimal](BigDecimal(0, mc))(_ + _)\n\nval runnableGraph = producer.via(consumers).toMat(resultAggregator)(Keep.right)\n\nval start = System.currentTimeMillis()\nval futureResult = runnableGraph.run()\n\nimport scala.concurrent.duration._\n\nval approximation = Await.result(futureResult, 1.minute)\n\nprintln(s\"final result: $approximation, time spent with stream ${System.currentTimeMillis() - start}ms\")\nactorSystem.terminate()\nFull source at GitHub\n最终打印结果为：\nfinal result: 0.693147680560195309417231996458176568325500134359192754120687759493393535594694716971113326967369656239812, time spent is 551ms\n可以看到，由akka stream实现起来，逻辑清晰又简单，并且效率得到了很高的提升，由1471ms提升到了551ms。\n至于如何实战Akka Stream，10月20日在上海会有一个Scala Meetup，我会在上面做一个Akka Stream In Action的分享，希望到时候关注。后续宣传图片和地点，会在本公众号上发布。","title":"Akka Stream实现一版上述逻辑"},{"location":"/2018/pricinpleOfReactiveStream.html#更多关于流量控制的内容","text":"反应式流的回压机制在《反应式设计模式》中被称为流量控制的一种。实际上，为了处理流量，还有更多内容要讲。但是一篇则会显得篇幅太长。有兴趣的同学可以等待《反应式设计模式》出版之后自行阅读，或者等我这个不知道什么时候心血来潮才会写一篇公众号的懒人更新后续内容。欢迎大家持续关注我的公众号，或者博客。我尽量不太懒。","title":"更多关于流量控制的内容"},{"location":"/2018/middleware_race_preliminary.html","text":"","title":"第四届阿里中间件性能大赛初赛RDP飞起来团队总结"},{"location":"/2018/middleware_race_preliminary.html#第四届阿里中间件性能大赛初赛rdp飞起来团队总结","text":"国庆刚刚好在弄阿里云组织的第一届POLARDB数据库性能大赛，忽然想起来可以把我之前写的阿里中间件比赛的总结发到博客里面。这一篇已经被几个公众号发过了，包括阿里中间件官方号。内容主要是如何使用akka来构建一个高性能的适配Dubbo的Service Mesh Sidecar。当然我的分数主要是通过Netty拿的。但是其实Netty只是胜在可以Zero Copy这一点上，贴近底层，没有什么意思。Akka构建Agent才包含更多的反应式思想。","title":"第四届阿里中间件性能大赛初赛RDP飞起来团队总结"},{"location":"/2018/middleware_race_preliminary.html#赛题背景分析及理解","text":"初赛题目是吸引我参加比赛的最大原因。其中一段描述了Service Mesh的作用:\n作为 Service Mesh 的核心组件之一，高性能的服务间代理（Agent）是不可或缺的，其可以做到请求转发、协议转换、服务注册与发现、动态路由、负载均衡、流量控制、降级和熔断等诸多功能，也是区别于传统微服务架构的重要特征。\n而这种思想与《反应式设计模式》不约而同。在反应式系统设计的过程中，很重要的一块就是如何与现存的非反应式系统进行交互。非反应式系统典型地都具有同步阻塞调用者、无界输入队列、不遵循有界响应延迟的原则等缺点，这使得流量控制、资源高效利用以及降级、熔断等功能都比较难以实现。《反应式设计模式》一书中专门推荐了要使用单独的资源来与这些系统整合，并赋予他们“反应式”的假象。而Service Mesh中的Agent，则可以看作成专门用来与非反应式系统进行整合的组件。在第14章的资源管理模式中，描述了如何使用这样的资源与之进行交互的方法，尤其是托管阻塞模式；而第16章的流量控制模式，则指导了我们如何在调用过程中行之有效进行流量控制。当然，对于比赛来说，这些设计相对来说过于概括。不过我们可以先基于这种概括性的原则构建出体系架构，之后我们再具体优化相关的细节，提高成绩。而基于之前描述的原因，我的第一版使用了Akka来进行开发。接下来我们先分析一下具体的题目。","title":"赛题背景分析及理解"},{"location":"/2018/middleware_race_preliminary.html#题目分析","text":"题目的要求是：\n实现一个高性能的 Service Mesh Agent 组件，并包含如下一些功能： 1. 服务注册与发现 2. 协议转换 3. 负载均衡\n服务注册与发现是为了获得资源的访问方式。这个过程最好不要与正式的调用过程耦合。所以我们用一个单独的Actor来做服务发现。如果是在Consumer中，这个Actor会去监听ETCD的变更，如果发现Endpoints发生了变化，则将信息发布到ActorSystem的事件流中。之后关注EndpointsUpdated事件的Actor就会收到此消息，并根据它来更新自己的端点列表，进行负载的动态变更。\n协议转换相对来说是一个打铁的活，根据Dubbo协议一点点写好就行了。\n重要的则在于负载均衡。进一步回到题目描述中：\n每轮评测在一台服务器中启动五个服务（以 Docker 实例的形式启动），一个 etcd 服务作为注册表、一个 Consumer 服务和三个 Provider 服务； 使用一台独立的施压服务器，分不同压力场景对 Consumer 服务进行压力测试，得到 QPS；\n总共有3轮压力测试，分别是128、256、512个连接。由于每次请求的往返时间最少也是50ms，那么每秒钟，按照512连接的最大速度，则是1000 / 50 * 512 = 10240的最大QPS。\n其中，三台Provider的负载能力有所不同，按照CPU的quota分配以及内存的大小分配，正常情况下应该是1比2比3。只是由于Provider的dubbo端最多同时只能处理200个请求，多出来的直接被reject掉。那么最好的分配比例在512条件下则是 112 : 200 : 200。\n当然，根据我们之前说的反应式流回压的原理，反应式系统的设计原则并不是固定分配比例的。它希望的理想情形是你告诉我你想要处理多少任务，一旦任务来了，我就尽量按照这个数量发给你。不要Consumer去强行推，不要Provider一直来拉。而这种模式最好的实现方式，就是利用Akka Stream。只不过，由于我们的工作其实是跨系统的，而Akka并没有在其TCP连接实现按照处理能力进行的回压机制，而是通过网络连接的缓冲区Buffer是否溢出而进行的回压，所以并非完美的反应式流机制。后续深入研究的时候我才知道了RSocket协议的存在以及rsocket-java。有兴趣的同学可以调研一下其性能。\n下面我们继续说比赛的内容。","title":"题目分析"},{"location":"/2018/middleware_race_preliminary.html#核心思路","text":"按照前面的分析，核心思路就是将每个Provider的处理过程看作是一条流。来自调用端的所有请求先汇聚到一个队列里面，之后根据后端Provider的处理能力，分别分配到三个不同的流中。而如果汇聚队列的长度达到了界限值，则降级服务，对外部请求进行按比例丢弃，直到与系统的处理能力重新匹配（详情参见《反应式设计模式》第十六章丢弃模式）。这样整个系统就又健壮又迅速。","title":"核心思路"},{"location":"/2018/middleware_race_preliminary.html#关键代码","text":"下面一段是用来抽象Consumer的Actor里面的代码，所有连接的请求都被注册到RequestHandler这个Actor了。\nval requestHandler: ActorRef = context.actorOf(Props(new RequestHandler).withDispatcher(\"mpsc\"), \"request-handler\")\n\n  def receive: Receive = {\n    case Bound(localAddress) ⇒\n      etcdManager ! \"consumer\"\n      log.info(s\"service started at ${localAddress.getHostString}:${localAddress.getPort}\")\n    case CommandFailed(_: Bind) ⇒\n      context stop self\n    case Connected(_, _) ⇒\n      val connection = sender()\n      connection ! Register(requestHandler) \n      //将Connection全部注册到RequestHandler，就是说所有连接发过来的数据都回转发到这个Actor\n      //注意这个是Hack写法。正统的还是应该一个Actor一个连接，这样逻辑才会清晰。\n  }\n然后在Requesthandler里面，接收到的ByteString直接作为元素提供给后面的处理流代码里面。\nvar source: SourceQueueWithComplete[(Long, ByteString)] = _\n  \n  override def receive: Receive = {\n    case Received(bs) =>\n      source.offer(sender().path.name.toLong, bs) //这里的sender是处理连接的actor，它们的名字刚刚好是ID，所以直接复用。\n    case EndpointsUpdate(newEndpoints) =>\n      log.info(s\"start new source for endpoints $newEndpoints\")\n      source.complete()\n      source = getSourceByEndpoints(newEndpoints)\n这里的source是一个可完成Source。Source, Flow, Sink是Akka Stream里面的基本构建块。其大体意义如下：\nSource: 只有一个输出流的构件块; Sink: 只接收一个输入流的构件块; Flow: 接收一个输入流，并拥有一个输出流的构件块。 Graph: 一个打包好的流处理拓扑，它可以拥有一组输入端口或者输出端口。\n我们这里是一个SourceQueueWithComplete，它由Source.queue声明并物化后产生：\ndef getSourceByEndpoints(endpoints: Set[Endpoint]): SourceQueueWithComplete[(Long, ByteString)] = {\n    val handleFlow = Flow[(Long, ByteString)]\n      .via(DubboFlow.connectionIdFlow)\n      .via(endpointsFlow(endpoints))\n      .to(DubboFlow.decoder)\n      \n    Source.queue[(Long, ByteString)](512, OverflowStrategy.backpressure)\n      .to(handleFlow).run()\n  }\n这里是由这个函数基于Endpoint的个数构建。第一段handleFlow是构建了一个Flow，这个Flow可以接收一个二元组(Long, ByteString)，并将其交给DubboFlow.connectionIdFlow来encode成自定义协议，之后将其发送到endpointsFlow进行对Provider的调用，并得到结果。得到结果之后，经由DubboFlow.decoder来decode，并发送回给各个连接Actor，由其返回给客户端。\n上面的内容里面，DubboFlow.connectionIdFlow和DubboFlow.decoder不多说，都是打铁代码。核心逻辑endpointsFlow(endPoints)贴出如下：\ndef endpointsFlow(endpoints: List[Endpoint]) = {\n    \n    val tcpFlows = endpoints.map { endpoint =>\n      Tcp().outgoingConnection(endpoint.host, endpoint.port).async\n    }\n    \n    val framing = Framing.lengthField(4, 12, Int.MaxValue, ByteOrder.BIG_ENDIAN)\n\n    Flow.fromGraph(GraphDSL.create(tcpFlows) { implicit builder =>\n      tcpFlows =>\n        import GraphDSL.Implicits._\n        \n        val balance = builder.add(Balance[ByteString](tcpFlows.size))\n        val bigMerge = builder.add(Merge[ByteString](tcpFlows.size))\n        tcpFlows.foreach { tcp =>\n          balance  ~> tcp ~> framing ~> bigMerge\n        }\n        \n        FlowShape(balance.in, bigMerge.out)\n    })\n  }\n每一个endpoint都被映射成为一个Tcp的Flow，通往Provider端。之后使用Akka Stream的DSL方法，构建了一个Graph。这个Graph用图形表示，其拓扑结果则是如下：\n+------> Small Provider +--------> Framing +-------+\n                      |                                                  |\n    Input             |                                                  |                Output\n+--------> Balancer ---------> Medium Provider+--------> Framing +----------------> Merge +-------->\n                      |                                                  |\n                      |                                                  |\n                      +------> Large Provider +--------> Framing +-------+\n数据由左边输入，经过Balancer，这个Balancer是由Akka Stream提供的现成组件，它可以将上游的元素路由到下游，其特性如下：\n一个Balance由一个in端口和2到多个out端口， 当任意下游端口停止回压之后，它输出元素到下游输出端口； 当下游所有端口都在回压的时候，它就回压上游； 当上游完成时，它也完成； 当其eagerCancel参数设置为true时，任意下游取消，则其也取消；设置为false的时候，当所有下游取消，它才取消。\n由上面的拓扑结构可以看到，当任意Provider向上游表示可以处理请求的时候，Balancer就会在有请求到来的时候，向其输出；Provider处理完的请求，经过TCP拆包过程之后，就合并到一起，交由下游的流继续处理。如此，只要连接有请求过来，那么整个流就能一直运转。这个过程中，即使某个通往provider的连接断掉了，Balancer也能继续将请求路由到其他两个连接上。而这个时候，负责服务发现的Actor就会发出EndpointsUpdated的消息，此时RequestHandler会进入第二个匹配，用新的Endpoint来更新我们的处理流：\ncase EndpointsUpdate(newEndpoints) =>\n      log.info(s\"start new source for endpoints $newEndpoints\")\n      source.complete()\n      source = getSourceByEndpoints(newEndpoints)\n注意这里的complete是表示流不再接收新的请求，这之前已经入队的请求仍然会继续完成，直到全部处理完毕。\nProvider的代码相对Consumer就简单很多：\nval handleFlow = Tcp().outgoingConnection(host, dubboPort).async\n\n  def startService: Future[Done] = {\n    Tcp().bind(host, port).runForeach { conn =>\n      conn.handleWith(handleFlow)\n    }\n  }\n它只需要将Consumer过来的连接转发给后端的Dubbo，或者为了性能原因，它需要将自定义协议包装成Dubbo协议，然后发过去，再将结果转回，即可。\n到这里，我们用了大约不到300行代码，就完成了初赛题目的所有要求。并且代码的普适性和健壮性都很不错，后续还能依据需求，快速地实现任意一端的限流要求(Flow[Request].throttle(...))，或者加入断路器，进行快速失败。\n这套代码在CPU资源充足的时候，例如在我本地(注意，已经按照docker参数限定了CPU quota和内存)，256连接的时候可以跑4960, 512的时候可以跑9500。\n然而线上则表现不好，分别最多4500和6400。这是为什么呢？\n经过查询源码以后发现，问题出现在这一段：\n@tailrec def innerRead(buffer: ByteBuffer, remainingLimit: Int): ReadResult =\n        if (remainingLimit > 0) {\n          // never read more than the configured limit\n          buffer.clear()\n          val maxBufferSpace = math.min(DirectBufferSize, remainingLimit)\n          buffer.limit(maxBufferSpace)\n          val readBytes = channel.read(buffer)\n          buffer.flip()\n\n          if (TraceLogging) log.debug(\"Read [{}] bytes.\", readBytes)\n          if (readBytes > 0) info.handler ! Received(ByteString(buffer)) //这一段\n\n          readBytes match {\n            case `maxBufferSpace` ⇒ if (pullMode) MoreDataWaiting else innerRead(buffer, remainingLimit - maxBufferSpace)\n            case x if x >= 0      ⇒ AllRead\n            case -1               ⇒ EndOfStream\n            case _ ⇒\n              throw new IllegalStateException(\"Unexpected value returned from read: \" + readBytes)\n          }\n        } else MoreDataWaiting\n其中info.handler ! Received(ByteString(buffer))是将SocketChannel接收到的数据复制成ByteString类型之后，再发送出去的，所以相当于是从堆外把数据复制了进来，于是整个流程都不是zero copy的。本来在正常的应用场景下，不ZC是必然的，因为应用服务器肯定要把数据读出来进行处理。但是在本次比赛的场景以及作为Proxy的情况下，这种复制就是非常昂贵的操作了，直接导致Akka版本的代码无法和Netty竞争，即使代码再精简，思想再精妙，也无法取得好的成绩。所以在第二个版本中，我换用了Netty来跑分。\nNetty版本下的核心代码，分ConsumerAgent和调用PrivderAgent的NettyClient列出如下：\nConsumerAgent:\noverride def channelRead(ctx: ChannelHandlerContext, msg: scala.Any): Unit = {\n    msg match {\n      case in: ByteBuf =>\n        val meshRequest = MeshRequest(cid) //cid是connectionId,在连接建立的时候获取\n        val maybeClient = ClientChannelHolder.clientChannelCache.get() //client的channel存在了ThreadLocal里面，直接通过ThreadLocal获取到channelHandler\n        maybeClient.writeAndFlush(meshRequest.toCustomProtocol(in), maybeClient.voidPromise()) //将流入的bytebuffer转变成自定义协议的格式，并直接向client的channel刷入数据\n        meshRequest.recycle //回收MeshRequest对象\n    }\n  }\nNettyClient:\noverride def channelRead(ctx: ChannelHandlerContext, msg: scala.Any): Unit = {\n          msg match {\n            case bs: ByteBuf =>\n              val cid = bs.getLong(4) //从ByteBuffer中获取connectionId\n              val resp = MeshResponse(cid) //包装成MeshResponse\n              val ch = ServerChannelHolder.serverChannelMap.get().get(cid) //根据connectionId获取这个连接的SocketChannel\n              if(ch != null) { //如果存在的话，则刷入响应\n                ch.writeAndFlush(resp.toHttpResponse(bs), ch.voidPromise())\n              }\n              resp.recycle //回收MeshResponse对象\n          }\n        }\n这个是我所发现的最短的路线。其中省略了路由的过程。整体的线程设置如下：\nval acceptorGroup = new EpollEventLoopGroup(1)\n  val threadFactory = new DefaultThreadFactory(\"atf_wrk\")\n  val workerNumber = 3\n  val workerGroup = new EpollEventLoopGroup(workerNumber, threadFactory)\n一个负责IO的线程，三个负责处理请求的线程。三个NettyClient分别使用三个worker中的一个就好了：\nval eventLoop = workerGroup.next()\n  new NettyClient(ed.host, ed.port, eventLoop, 1, ed.scale)\n主要的trick就是我只起了4个线程，1个负责IO，3个负责请求处理。通过连接绑定的线程来进行路由，所以少了很多人加权轮询的步骤，而且每个连接只通过同一个线程进行流转，所以也少了context switch的过程。情况好的话，4个线程应该pin到它们的cpu上，没有任何的上下文切换。\n至于其他就是一些打铁的小细节，比如使用Recycler生成对象池来回收对象，使用池化的ByteBuf来避免堆外内存分配的开销，预先定义好一些要用来包装请求和回复的对象，使用Unpooled.unreleasableBuffer(buffer)来反复利用。如此，整个过程下来，不会有FGC，而YGC最多也就两三次而已。Recycler的代码列出如下：\nclass MeshRequest private(handle: Handle[MeshRequest]) {\n  private var cid: Long = _\n  private var buffer: ByteBuf = _\n  private var composite: CompositeByteBuf = _\n\n  def recycle = {\n    cid = 0l\n    buffer = null\n    composite = null\n    handle.recycle(this)\n  }\n\n  def toCustomProtocol(bb: ByteBuf) = {\n    val n = bb.indexOf(280, bb.readableBytes(), '='.toByte)\n    val parameter = bb.skipBytes(n + 1)\n    buffer.writeLong(cid)\n    buffer.writeInt(parameter.readableBytes())\n    composite.addComponents(true, buffer, parameter)\n  }\n\n}\n\nobject MeshRequest {\n  private val RECYCLER = new Recycler[MeshRequest]() {\n    override def newObject(handle: Recycler.Handle[MeshRequest]): MeshRequest = {\n      new MeshRequest(handle)\n    }\n  }\n\n  def apply(id: Long): MeshRequest = {\n    val request = RECYCLER.get()\n    request.cid = id\n    request.buffer = ConsumerAgent.allocator.directBuffer(12)\n    request.composite = ConsumerAgent.allocator.compositeBuffer()\n    request\n  }\n}\n最终，Netty版本的代码停留在6894，而Akka版本我没记错的话，应该是6400左右。","title":"关键代码"},{"location":"/2018/middleware_race_preliminary.html#比赛经验总结和感想","text":"其实是第一次参加这种编程的比赛，开始的时候看得蛮轻，因为按照一般的生产场景来说，我的第一种方案已经足够好了，编码简单、健壮、可扩展性强，应该是能够出彩的。但是因为比赛的场景和需求不同，所以非ZC的方案没办法取得好的成绩，所以后续不得已，只能放弃Akka，使用Netty写了一个版本的打铁代码，以往前冲击一个比较好的名次，然后争取机会来向大家吹嘘Akka。事实证明，限定场景来做极致优化的话，Netty确实好很多，不过，在通用场景下，用Akka Stream的思想，则可以迅速构建出一个集各种流控功能于一体，也非常好扩展，并且性能也不会相差太多的组件。所以，不管怎样，到最终的总结还是，如果是需要读取出数据的应用服务场景下，我来开发的话，Akka和Akka Stream绝对会是主力，而Netty则可以被应用在不需要将数据读出内存的场景(如只负责转发或者解析自定义协议的Provider端)。两者相结合，应该可以达到比较好的平衡。","title":"比赛经验总结和感想"},{"location":"/2018/jmh.html","text":"","title":"应用JMH测试大型HashMap的性能"},{"location":"/2018/jmh.html#应用jmh测试大型hashmap的性能","text":"","title":"应用JMH测试大型HashMap的性能"},{"location":"/2018/jmh.html#polardb初赛进展","text":"写这篇是因为PolarDB比赛很重要的一点是控制内存。C++只有2G，Java也只有3G，而6400W的键值对，即使只是Long类型，也需要16 * 64 * 10e6 ≈ 1G的内存，这还不包括其他对象引用的相关开销，所以内存控制在这里是非常重要的，因为稍不小心就会被CGroup无情地kill掉。因此在比赛开始没多久的时候我就研究了一下使用怎样的HashMap可以达到内存最简的状况。在这个过程中，顺便使用了JMH来分析了一下几个侯选库的性能。因为初赛相对来说比较简单，而且HashMap实际上在复赛时候的Range操作上没有发挥余地，所以我决定将这篇写下来分享给大家，希望能帮助更多对比赛有兴趣的同学找到一个比较好的入手点。\n之前的初赛简单思路可以看这里。","title":"PolarDB初赛进展"},{"location":"/2018/jmh.html#侯选的集合库","text":"我们能第一时间想到的最朴素最直接的候选者就是Java自带的HashMap了，这是我们平时使用最多也是最熟悉的实现。只不过在这里因为性能和内存消耗的原因，它稍微有点不合适。其实市面上有很多其他优秀的集合库实现的，我在这里大致列一下我这边会测试的几个：\nFastUtil: 一个意大利的计算机博士开发的集合库。 Eclipse Collections: 由高盛开发的集合库，后来捐给了eclipse基金会，成为了eclipse的项目. HPPC: 专门为原始类型设计的集合库。 Koloboke: 又一位大神的作品，目标是低内存高性能。 Trove: 挂在bitbucket上面的一个开源项目。\n因为是为了比赛而接触的这些库，所以我只按照比赛场景给他们做了测试。我们会预生成6400W对8字节的Key，和8字节的长整型Value，之后会将这些key全部写入各自的HashMap中去，然后再从中读取出来，并与暂存的Value作比较，判断正确性。整个的测试过程是交给JMH来做的。下面介绍一下JMH工具。","title":"侯选的集合库"},{"location":"/2018/jmh.html#jmh简介","text":"JMH是由OpenJDK开发的，用来构建、运行和分析Java或其他Jvm语言所写的程序的基准测试框架。它可以帮助我们自动构建和运行基准测试，并且汇总得到结果。现在一般Java世界里面的主流Benchmark就是应用的JMH。\nScala这边，我们所熟悉的Ktoso大佬包了一个sbt-jmh插件，使得我们可以方便地利用SBT来运行JMH测试。要使用sbt-jmh插件，首先，在plugins.sbt文件里面添加插件：\n// project/plugins.sbt\naddSbtPlugin(\"pl.project13.scala\" % \"sbt-jmh\" % \"0.3.4\")\n之后，在项目中的模块定义中，使用它：\n// build.sbt\nenablePlugins(JmhPlugin)\n然后，我们就可以在sbt的console下，执行如下命令，来运行jmh测试了：\njmh:run -i 3 -wi 3 -f1 -t1 .*FalseSharing.*\n上面的参数解释下来，就是要求项目对符合正则表达式.*FalseSharing.*的基准测试，运行3次，运行之前要进行3次预热，只需要跑一遍，使用一个线程。\n好，介绍结束，我们接下来看一下我们如何来编写程序测试各种Map。","title":"JMH简介"},{"location":"/2018/jmh.html#hashmap测试代码","text":"首先，我们先创建一个类，如下：\n@State(Scope.Benchmark)\n@OutputTimeUnit(TimeUnit.SECONDS)\n@BenchmarkMode(Array(Mode.Throughput))\nclass jmh.HashMapBenchmark {\n\n}\nJMH使用注解来标记的基准测试。上面三个注解的选项的意思分别是：\nState表明可以在类里面创建成员变量，供所有测试复用，复用的范围是在Benchmark当中； OutputTimeUnit表示输出Benchmark结果的时候，计时单位是TimeUnit.SECONDS； BenchmarkMode，Benchmark的模式是测试吞吐率。\n为了做基准测试，我们首先创建一个6400W大小的列表，列表的元素是一个二元组，都是Long:\nval random = new Random(42)\n  \n  val testSet: List[(Long, Long)] = List.range(0, 64000000).map { _ =>\n    val key = random.nextLong()\n    val value = random.nextLong()\n    key -> value\n  }\n这里有个关于比赛的小技巧，由于Key都是8字节的，所以其实每个Key都很容易转化成Long类型的。所以我们在测试里面也只测试对于Long类型的写入性能，以Java的HashMap为例：\n@Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testHashMap() = {\n    val hashMap = new util.HashMap[Long, Long](64000000)\n    testSet.foreach { case (k, v) =>\n      hashMap.put(k, v)\n    }\n    testSet.foreach { case (k, v) =>\n      val readValue = hashMap.get(k)\n      assert(readValue == v)\n    }\n  }\n@Benchmark表示这是一个要运行的基准测试。@OperationsPerInvocation注解会接收一个数值，表示这个测试的方法运行了多少次。在我们这边是OperationPerInvocation次。注意，前面的变量在jmh.HashMapBenchmark的伴生对象中定义：\nobject jmh.HashMapBenchmark {\n  val OperationsPerInvocation = 64000000 * 2\n}\n然后，我们就能够启动sbt，输入前面提到的jmh:run命令了。我们先跑一波看看：\njmh:run -i 3 -wi 3 -f1 -t1 .*HashMap.*\n跑起来以后我感觉我错了，电脑风扇在狂转，而且预热半天都跑不完。jstat看一下gc情况试试先，发现100多秒都是FGC。。\nS0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT\n931840.0 931840.0  0.0    0.0   932352.0 932352.0 5592576.0  5592545.4  9856.0 9391.3 1408.0 1316.6     12   36.611   9    145.669  182.280\n931840.0 931840.0  0.0    0.0   932352.0 932352.0 5592576.0  5592545.4  9856.0 9391.3 1408.0 1316.6     12   36.611   9    145.669  182.280\n果断杀掉，加上jvm参数以后再测试：\njmh:run -i 3 -wi 3 -f1 -t1 --jvmArgs \"-Xmx10g\" .*HashMap.*\n10G也是慢，跑太久了，不乐意跑了，果断放弃。我们直接来看其他的实现。\n这里还要说一下，因为内存有要求，所以我们需要同时打印一下HashMap的内存大小。我所使用的是网上找到的一个应该是从Spark代码中抠出来的一个实现，速度快，估值准。只需要在build.sbt中如下引入即可。\nlibraryDependencies += \"com.madhukaraphatak\" %% \"java-sizeof\" % \"0.1\"","title":"HashMap测试代码"},{"location":"/2018/jmh.html#主要代码编写","text":"因为其实这里的hashmap的库使用其实大同小异，为了避免重复，所以我利用Scala的一些特性来进行代码编写。首先我定义了一个特质为LongLongOp:\ntrait LongLongOp {\n  def put(key:Long, value:Long):Long\n  def get(key:Long):Long\n}\n之后，我们写一个遍历testSet的函数：\ndef testSetTraverse(hashMap:LongLongOp) = {\n     testSet.foreach { case (k, v) =>\n      hashMap.put(k, v)\n    }\n    testSet.foreach { case (k, v) =>\n      val readValue = hashMap.get(k)\n      assert(readValue == v)\n    }\n  }\n然后，我们利用特质可以混入的特性，在生成HashMap的时候，混入LongLongOp，然后交由testSetTraverse执行。这样我们每次只需要新建HashMap即可了。例如，FastUtil的测试如下：\n@Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testFastUtil() = {\n    val map = new Long2LongArrayMap(MapSize) with LongLongOp\n    testSetTraverse(map)\n    printlnObjectSize(\"fastutil Long2LongArrayMap\", map)\n  }\n其中printLnObjectSize是用来打印map占用内存数量的：\ndef printlnObjectSize(message:String, obj: AnyRef) = {\n    val size = SizeEstimator.estimate(obj)\n    val sizeInGB = size.toDouble / 1024 / 1024 / 1024\n    println(s\"$message size is ${sizeInGB}GB\")\n  }\n之后，依次使用Eclipse Collections, Hppc, Koloboke和Trove，就完成了我们的Benchmark编写：\nimport java.util.concurrent.TimeUnit\n\nimport com.koloboke.collect.impl.hash.MutableLHashParallelKVLongLongMapGO\nimport com.madhukaraphatak.sizeof.SizeEstimator\nimport gnu.trove.map.hash.TLongLongHashMap\nimport org.openjdk.jmh.annotations._\n\nimport scala.util.Random\n\nobject HashMapBenchmark {\n  final val OperationsPerInvocation = 64000000 * 2\n}\n\ntrait LongLongOp {\n  def put(key: Long, value: Long): Long\n\n  def get(key: Long): Long\n}\n\n@State(Scope.Benchmark)\n@OutputTimeUnit(TimeUnit.SECONDS)\n@BenchmarkMode(Array(Mode.Throughput))\nclass HashMapBenchmark {\n\n  import HashMapBenchmark._\n\n  val random = new Random(42)\n  val MapSize = 64000000\n\n  val testSet: List[(Long, Long)] = List.range(0, MapSize).map { _ =>\n    val key = random.nextLong()\n    val value = random.nextLong()\n    key -> value\n  }\n\n  def testSetTraverse(hashMap: LongLongOp) = {\n    testSet.foreach { case (k, v) =>\n      hashMap.put(k, v)\n    }\n    testSet.foreach { case (k, v) =>\n      val readValue = hashMap.get(k)\n      assert(readValue == v)\n    }\n  }\n\n  def printlnObjectSize(message: String, obj: AnyRef) = {\n    val size = SizeEstimator.estimate(obj)\n    val sizeInGB = size.toDouble / 1024 / 1024 / 1024\n    println(s\"$message size is ${sizeInGB}GB\")\n  }\n\n  @Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testEclipseCollection() = {\n\n    import org.eclipse.collections.impl.map.mutable.primitive.LongLongHashMap\n\n    val map = new LongLongHashMap(MapSize)\n    testSet.foreach { case (k, v) =>\n      map.put(k, v)\n    }\n    testSet.foreach { case (k, v) =>\n      val readValue = map.get(k)\n      assert(readValue == v)\n    }\n    printlnObjectSize(\"Eclipse LongLongHashMap\", map)\n  }\n\n  @Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testHppc() = {\n\n    import com.carrotsearch.hppc.LongLongHashMap\n\n    val map = new LongLongHashMap(MapSize, 0.99) with LongLongOp\n    testSetTraverse(map)\n    printlnObjectSize(\"hppc LongLongHashMap\", map)\n  }\n\n  @Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testKoloboke() = {\n    val map = new MutableLHashParallelKVLongLongMapGO() with LongLongOp\n    testSetTraverse(map)\n    printlnObjectSize(\"Koloboke\", map)\n  }\n\n  @Benchmark\n  @OperationsPerInvocation(OperationsPerInvocation)\n  def testTrove() = {\n    val map = new TLongLongHashMap(MapSize, 1.0f) with LongLongOp\n    testSetTraverse(map)\n    printlnObjectSize(\"Trove LongLongMap\", map)\n  }\n}\n其中Eclipse collection的put方法的返回值是void，与其他集合不一样，所以只能单独为它写一个测试方法。","title":"主要代码编写"},{"location":"/2018/jmh.html#结果","text":"运行的过程中，Koloboke报一个诡异的空指针错误，所以没有通过测试；FastUtils在这个量级好像有点慢，不乐意等所以最终没有把它加入测试。最终我们得到如下的结果列表：\n集合库 类型 capacity loadFactor 内存占用 ops Eclipse Collection LongLongHashMap 64000000 默认值，难修改 2.01G 9243033.271 ops/s HPPC LongLongHashMap 64000000 0.99 1.0G 8172318.238 ops/s Trove TLongLongHashMap 64000000 1.0 1.08G 4211399.836 ops/s\n综合内存使用以及性能，我个人觉得在此次比赛初赛中，也许HPPC是个比较好的选择。\n所以，初赛使用Java的HashMap实现的小伙伴，是不是应该赶紧思考一下换一下内存索引的结构，来避免OOM呢？","title":"结果"}]}